{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What I want to do: ** This is my attempt at file handling. Should be fun! Prototype the construction of the function that splits an image dataset into test, train, validation and sample datasets in the following format [thanks VedAustin](https://gist.githubusercontent.com/VedAustin/a6bdc1ea5dcc1c363053b1dd1b16e88c/raw/960b61ea0a5eec73563526afbaa17bbb9f1ecd08/create_folders.py):\n",
    "\n",
    "                    \t\t\t\t\t    data  \n",
    "                 ____________________________||__________________\n",
    "                 |\t      |\t              |\t              |\n",
    "               train\ttest  \t          valid              sample\t\t \n",
    "\t          ____|___                   _____|___          ______|_______     \n",
    "              |       |                 |         |        |              |\n",
    "           cats    dogs               cats     dogs    train            valid\n",
    "                                               \t     ___|___          ____|____    \n",
    "                                               \t    |       |        |         |\n",
    "                                             \t    cats   dogs     cats      dogs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Originally, image data from kaggle is in the format:<br>\n",
    "\n",
    "                    \t\t\t\t\t    data  \n",
    "                 ____________________________||__________________\n",
    "                 |\t      |\t              \n",
    "               train\ttest\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1 is to introduce a sample directory and copy 1% sample of images from the training folder for building models on a non-GPU laptop<br>\n",
    "\n",
    "\n",
    "                    \t\t\t\t\t    data  \n",
    "                 ____________________________||__________________\n",
    "                 |\t      |\t              |\t              \n",
    "               train\ttest  \t          sample\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2 is to build a validation set using 80:20 rule or any user-defined split.<br>\n",
    "Step 3 is to classify images in training and validation directories created in Step 2 into constituent classes<br>\n",
    "\n",
    "                    \t\t\t\t\t    data  \n",
    "                 ____________________________||__________________\n",
    "                 |\t      |\t              |\t              |\n",
    "               train\ttest  \t          valid              sample\t\t \n",
    "\t          ____|___                   _____|___          \n",
    "              |       |                 |         |        \n",
    "           cats    dogs               cats     dogs    \n",
    "           \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4 is to run steps 2 and 3 in our sample directory as well. Thats it!<br>\n",
    "\n",
    "                    \t\t\t\t\t    data  \n",
    "                 ____________________________||__________________\n",
    "                 |\t      |\t              |\t              |\n",
    "               train\ttest  \t          valid              sample\t\t \n",
    "\t          ____|___                   _____|___          ______|_______     \n",
    "              |       |                 |         |        |              |\n",
    "           cats    dogs               cats     dogs    train            valid\n",
    "                                               \t     ___|___          ____|____    \n",
    "                                               \t    |       |        |         |\n",
    "                                             \t    cats   dogs     cats      dogs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Admin stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "from random import shuffle\n",
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split any folder into constituent classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define classes in this case as a list of \"thumb\" and \"index\". The big idea here is to pass this list of classes into the function, and it searches each file name within the directory to classify. Finally, the function will also create directories with the names of the classes and move the images into these newly created directories accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sourcepath = \"data/train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# a list of classes will be passed into the function\n",
    "response_classes = ['thumb','index']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, dynamically create child directories named as per the response classes. These will be your destination paths. I will store the class as well as the destination path for these classes in a dict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'index': 'data/train/index', 'thumb': 'data/train/thumb'}\n"
     ]
    }
   ],
   "source": [
    "# Next, dynamically create child directories named as per the response classes. These will be your destination paths.\n",
    "destpath={}\n",
    "for cl in response_classes:\n",
    "    destpath[cl] = os.path.join(sourcepath,cl)\n",
    "    # to move files into directories, we need to first create the destination directories. Create new only if doesnt \n",
    "    # exist, otherwise it throws an 'os' package error\n",
    "    if not os.path.isdir(destpath[cl]):\n",
    "        os.mkdir(destpath[cl])\n",
    "print destpath"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the shutil module offers a number of high-level operations on files and collections of files. In particular, functions are provided which support file copying and removal. For operations on individual files, see also the os module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# look into each file name. then classify them into index or thumb and then move them into appropriate destinations.\n",
    "\n",
    "# look into each of them and classify. Obviously move only the ones that belong to one or the other classes. \n",
    "# Print out the number of exceptions so that I can go and manually look at it. If it is a file like .DS_Store, then\n",
    "# I can ignore it.\n",
    "# IMPORTANT NOTE: BEFORE MOVING THE FILES, ENSURE THAT THE DESTINATION DIRECTORIES ARE PRESENT\n",
    "for f in os.listdir(sourcepath):\n",
    "    for item in destpath:\n",
    "        if item in f:\n",
    "            shutil.move(os.path.join(sourcepath,f), destpath[item])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, check if any files have been left unprocessed. This should not happen unless there are some data issues. So best to point it out and leave it to your friendly data scientist to figure out. In the example below, I've manually changed the filenames for 2 files, thats why they've gotten left out along with the .DS_Store file. Worth checking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.DS_Store',\n",
       " 'index',\n",
       " 'P9Ubq5JZTviNEMvxWyYyUg_villa_62a.jpg',\n",
       " 'thumb',\n",
       " 'zodRD52bSn6VlzxOw+Oh7w_voti_62f.jpg']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unprocessed_files = os.listdir(sourcepath)\n",
    "unprocessed_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for item in destpath:\n",
    "    unprocessed_files.remove(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Check filepath data/train - some files could not be classified into response_classes.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-35244995b1e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munprocessed_files\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0;34m\"Check filepath \"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0msourcepath\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\" - some files could not be classified into response_classes.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m: Check filepath data/train - some files could not be classified into response_classes."
     ]
    }
   ],
   "source": [
    "assert len(unprocessed_files) == 0 ,\"Check filepath \"+sourcepath+\" - some files could not be classified into response_classes.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tying it all into a function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_inputs_into_response_classes(sourcepath, response_classes):\n",
    "    '''\n",
    "    Define classes in this case as a list of \"thumb\" and \"index\". \n",
    "    The big idea here is to pass this list of classes into the function, \n",
    "    and it searches each file name within the directory to classify. \n",
    "    Finally, the function will also create directories with the names \n",
    "    of the classes and move the images into these newly created directories accordingly.\n",
    "    '''\n",
    "    # Dynamically create child directories named as per the response classes. These will be your destination paths.\n",
    "    destpath={}\n",
    "    for cl in response_classes:\n",
    "        destpath[cl] = os.path.join(sourcepath,cl)\n",
    "        # to move files into directories, we need to first create the destination directories. Create new only if doesnt \n",
    "        # exist, otherwise it throws an 'os' package error\n",
    "        if not os.path.isdir(destpath[cl]):\n",
    "            os.mkdir(destpath[cl])\n",
    "\n",
    "    # look into each file name. then classify them into index or thumb and then move them into appropriate destinations.\n",
    "    # look into each of them and classify. Obviously move only the ones that belong to one or the other classes. \n",
    "    # Print out the number of exceptions so that I can go and manually look at it. If it is a file like .DS_Store, then\n",
    "    # I can ignore it.\n",
    "    # IMPORTANT NOTE: BEFORE MOVING THE FILES, ENSURE THAT THE DESTINATION DIRECTORIES ARE PRESENT\n",
    "    for f in os.listdir(sourcepath):\n",
    "        for item in destpath:\n",
    "            if item in f:\n",
    "                shutil.move(os.path.join(sourcepath,f), destpath[item])\n",
    "            \n",
    "    # Check if any files have been left unprocessed. This should not happen unless there are some data issues. \n",
    "    # So best to point it out and leave it to your friendly data scientist to figure out.\n",
    "    unprocessed_files = os.listdir(sourcepath)\n",
    "    for item in destpath:\n",
    "        unprocessed_files.remove(item)\n",
    "    \n",
    "    return len(unprocessed_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check = split_inputs_into_response_classes(\"data/train\", ['divvy','poovy','vivvy'])\n",
    "check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a % split into training and validation folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "perc_split = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sourcepath = \"data/train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['%sGrCwA+SFWLLhXrdXsJnw_divvy_62e.jpg',\n",
       " '+Ia1w3OBRfG+wt7jSe3LNw_poovy_632.jpg',\n",
       " '.DS_Store',\n",
       " '2FVCA93xQYmDLdep7QmhDg_poovy_629.jpg',\n",
       " '55jx2rgoROSA57h%DCxydQ_divvy_62b.jpg',\n",
       " '5Lhtyh21SDaWmhgzVOsdSQ_divvy_62d.jpg',\n",
       " 'FiRg4dgfTJK5%zyn2bNWAQ_poovy_636.jpg',\n",
       " 'iHPIlCiPQmKu5Pxl6YpgGw_poovy_63b.jpg',\n",
       " 'iSpztfhwQc+TaXsIrhjOsw_divvy_63a.jpg',\n",
       " 'kQeWq2EVRn6EhgHMasQ3Eg_poovy_634.jpg',\n",
       " 'luXr4dzgTAq%Y6p+B6Iafw_vivvy_630.jpg',\n",
       " 'P9Ubq5JZTviNEMvxWyYyUg_villavivvy_62a.jpg',\n",
       " 'PLk59%R0RiOGpB7oLmRdwQ_index_63dvivvy.jpg',\n",
       " 's4kbUx5YQfy9z19qGmPXxg_thumb_631vivvy.jpg',\n",
       " 'sUFwgkMZTumyur0gLg1ySQ_vivvy_635.jpg',\n",
       " 'sujULyDVRxCn20jEWrBfEQ_vivvy_633.jpg',\n",
       " 'UNADJUSTEDNONRAW_divvy_638.jpg',\n",
       " 'UNADJUSTEDNONRAW_divvy_639.jpg',\n",
       " 'WzD8UwhJRDawTTft74AhXA_poovy_62c.jpg',\n",
       " 'xYJXpVP1Q+CLwYRppVx1xQ_poovy_637.jpg',\n",
       " 'zodRD52bSn6VlzxOw+Oh7w_vivvy_62f.jpg']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = os.listdir(sourcepath)\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['WzD8UwhJRDawTTft74AhXA_poovy_62c.jpg',\n",
       " '+Ia1w3OBRfG+wt7jSe3LNw_poovy_632.jpg',\n",
       " '55jx2rgoROSA57h%DCxydQ_divvy_62b.jpg',\n",
       " 'PLk59%R0RiOGpB7oLmRdwQ_index_63dvivvy.jpg',\n",
       " 'luXr4dzgTAq%Y6p+B6Iafw_vivvy_630.jpg',\n",
       " 'xYJXpVP1Q+CLwYRppVx1xQ_poovy_637.jpg',\n",
       " 'sUFwgkMZTumyur0gLg1ySQ_vivvy_635.jpg',\n",
       " 'UNADJUSTEDNONRAW_divvy_638.jpg',\n",
       " 'zodRD52bSn6VlzxOw+Oh7w_vivvy_62f.jpg',\n",
       " 'P9Ubq5JZTviNEMvxWyYyUg_villavivvy_62a.jpg',\n",
       " 'sujULyDVRxCn20jEWrBfEQ_vivvy_633.jpg',\n",
       " 'FiRg4dgfTJK5%zyn2bNWAQ_poovy_636.jpg',\n",
       " 'iSpztfhwQc+TaXsIrhjOsw_divvy_63a.jpg',\n",
       " 'iHPIlCiPQmKu5Pxl6YpgGw_poovy_63b.jpg',\n",
       " 'kQeWq2EVRn6EhgHMasQ3Eg_poovy_634.jpg',\n",
       " '2FVCA93xQYmDLdep7QmhDg_poovy_629.jpg',\n",
       " '.DS_Store',\n",
       " 'UNADJUSTEDNONRAW_divvy_639.jpg',\n",
       " '%sGrCwA+SFWLLhXrdXsJnw_divvy_62e.jpg',\n",
       " 's4kbUx5YQfy9z19qGmPXxg_thumb_631vivvy.jpg',\n",
       " '5Lhtyh21SDaWmhgzVOsdSQ_divvy_62d.jpg']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shuffle(files)\n",
    "shuffle(files)\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(len(files)*perc_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "split_train = files[:int(len(files)*perc_split)]\n",
    "split_valid = files[int(len(files)*perc_split):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['WzD8UwhJRDawTTft74AhXA_poovy_62c.jpg',\n",
       " '+Ia1w3OBRfG+wt7jSe3LNw_poovy_632.jpg',\n",
       " '55jx2rgoROSA57h%DCxydQ_divvy_62b.jpg',\n",
       " 'PLk59%R0RiOGpB7oLmRdwQ_index_63dvivvy.jpg',\n",
       " 'luXr4dzgTAq%Y6p+B6Iafw_vivvy_630.jpg',\n",
       " 'xYJXpVP1Q+CLwYRppVx1xQ_poovy_637.jpg']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sUFwgkMZTumyur0gLg1ySQ_vivvy_635.jpg',\n",
       " 'UNADJUSTEDNONRAW_divvy_638.jpg',\n",
       " 'zodRD52bSn6VlzxOw+Oh7w_vivvy_62f.jpg',\n",
       " 'P9Ubq5JZTviNEMvxWyYyUg_villavivvy_62a.jpg',\n",
       " 'sujULyDVRxCn20jEWrBfEQ_vivvy_633.jpg',\n",
       " 'FiRg4dgfTJK5%zyn2bNWAQ_poovy_636.jpg',\n",
       " 'iSpztfhwQc+TaXsIrhjOsw_divvy_63a.jpg',\n",
       " 'iHPIlCiPQmKu5Pxl6YpgGw_poovy_63b.jpg',\n",
       " 'kQeWq2EVRn6EhgHMasQ3Eg_poovy_634.jpg',\n",
       " '2FVCA93xQYmDLdep7QmhDg_poovy_629.jpg',\n",
       " '.DS_Store',\n",
       " 'UNADJUSTEDNONRAW_divvy_639.jpg',\n",
       " '%sGrCwA+SFWLLhXrdXsJnw_divvy_62e.jpg',\n",
       " 's4kbUx5YQfy9z19qGmPXxg_thumb_631vivvy.jpg',\n",
       " '5Lhtyh21SDaWmhgzVOsdSQ_divvy_62d.jpg']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filesplit={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filesplit['train'] = files[:int(len(files)*perc_split)]\n",
    "filesplit['valid'] = files[int(len(files)*perc_split):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': ['WzD8UwhJRDawTTft74AhXA_poovy_62c.jpg',\n",
       "  '+Ia1w3OBRfG+wt7jSe3LNw_poovy_632.jpg',\n",
       "  '55jx2rgoROSA57h%DCxydQ_divvy_62b.jpg',\n",
       "  'PLk59%R0RiOGpB7oLmRdwQ_index_63dvivvy.jpg',\n",
       "  'luXr4dzgTAq%Y6p+B6Iafw_vivvy_630.jpg',\n",
       "  'xYJXpVP1Q+CLwYRppVx1xQ_poovy_637.jpg'],\n",
       " 'valid': ['sUFwgkMZTumyur0gLg1ySQ_vivvy_635.jpg',\n",
       "  'UNADJUSTEDNONRAW_divvy_638.jpg',\n",
       "  'zodRD52bSn6VlzxOw+Oh7w_vivvy_62f.jpg',\n",
       "  'P9Ubq5JZTviNEMvxWyYyUg_villavivvy_62a.jpg',\n",
       "  'sujULyDVRxCn20jEWrBfEQ_vivvy_633.jpg',\n",
       "  'FiRg4dgfTJK5%zyn2bNWAQ_poovy_636.jpg',\n",
       "  'iSpztfhwQc+TaXsIrhjOsw_divvy_63a.jpg',\n",
       "  'iHPIlCiPQmKu5Pxl6YpgGw_poovy_63b.jpg',\n",
       "  'kQeWq2EVRn6EhgHMasQ3Eg_poovy_634.jpg',\n",
       "  '2FVCA93xQYmDLdep7QmhDg_poovy_629.jpg',\n",
       "  '.DS_Store',\n",
       "  'UNADJUSTEDNONRAW_divvy_639.jpg',\n",
       "  '%sGrCwA+SFWLLhXrdXsJnw_divvy_62e.jpg',\n",
       "  's4kbUx5YQfy9z19qGmPXxg_thumb_631vivvy.jpg',\n",
       "  '5Lhtyh21SDaWmhgzVOsdSQ_divvy_62d.jpg']}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filesplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create child directories for training and validation if it doesnt already exist.\n",
    "destpath=['train','valid']\n",
    "\n",
    "for folders in destpath:\n",
    "    # to move files into directories, we need to first create the destination directories. Create new only if doesnt \n",
    "    # exist, otherwise it throws an 'os' package error\n",
    "    if not os.path.isdir(os.path.join(sourcepath,folders)):\n",
    "        os.mkdir(os.path.join(sourcepath,folders))\n",
    "    \n",
    "    # interesting way to move files from directory A to directory B using OS.RENAME method \n",
    "    # I got this idea from http://stackoverflow.com/questions/39210765/randomly-distribute-files-into-train-test-given-a-ratio\n",
    "    for files in filesplit[folders]:\n",
    "        os.rename(sourcepath+'/'+files , sourcepath+'/'+folders+'/'+files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tying it all into a function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_into_training_validation_datasets(sourcepath, response_classes, perc_split=0.8):\n",
    "    '''\n",
    "    Split files in any directory randomly into training and validation datasets based on \n",
    "    the percentage split. Usually, we'll keep it as 80% split.\n",
    "    \n",
    "    requires 'shuffle' from the random package: from random import shuffle\n",
    "    '''\n",
    "    # Get a list of all the filenames in the directory into a list and randomly shuffle them\n",
    "    files = os.listdir(sourcepath)\n",
    "    shuffle(files)\n",
    "    shuffle(files)\n",
    "    \n",
    "    # split the filenames into train and validation based on the percentage split and \n",
    "    # store the filenames in a dict\n",
    "    filesplit={}\n",
    "    filesplit['train'] = files[:int(len(files)*perc_split)]\n",
    "    filesplit['valid'] = files[int(len(files)*perc_split):]\n",
    "    \n",
    "    # Create child directories for training and validation if it doesnt already exist.\n",
    "    destpath=['train','valid']\n",
    "\n",
    "    for folders in destpath:\n",
    "        # to move files into directories, we need to first create the destination directories. Create new only if doesnt \n",
    "        # exist, otherwise it throws an 'os' package error\n",
    "        if not os.path.isdir(os.path.join(sourcepath,folders)):\n",
    "            os.mkdir(os.path.join(sourcepath,folders))\n",
    "    \n",
    "        # interesting way to move files from directory A to directory B using OS.RENAME method \n",
    "        # I got this idea from http://stackoverflow.com/questions/39210765/randomly-distribute-files-into-train-test-given-a-ratio\n",
    "        for files in filesplit[folders]:\n",
    "            os.rename(sourcepath+'/'+files , sourcepath+'/'+folders+'/'+files)\n",
    "    \n",
    "        # call the function to split into response classes\n",
    "        num_unprocessed_files = split_inputs_into_response_classes(os.path.join(sourcepath,folders), response_classes)\n",
    "        print (\"%s files were unable to be classified into response classes in the '%s' sub-directory within the '%s' directory\" %(num_unprocessed_files,folders,sourcepath))\n",
    "        \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 files were unable to be classified into response classes in the 'train' sub-directory within the 'data/train' directory\n",
      "0 files were unable to be classified into response classes in the 'valid' sub-directory within the 'data/train' directory\n"
     ]
    }
   ],
   "source": [
    "split_into_training_validation_datasets(\"data/train\",['divvy','poovy','vivvy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.DS_Store', 'divvy', 'poovy', 'vivvy']"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(\"data/train/train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm. The DS_Store files and hidden files are being naughty and causing issues. Best to ignore these."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Copy 100 images or 1% of total images (whichever is smaller) into a sample dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sourcepath = \"data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    # the big idea is to copy from the train folder itself. Thats how kaggle is aligning these folders\n",
    "    # Get a list of all the filenames in the directory into a list and randomly shuffle them\n",
    "    # one can see that kaggle puts bulk of the images in the training directory\n",
    "    files = os.listdir(os.path.join(sourcepath,'train'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['%sGrCwA+SFWLLhXrdXsJnw_divvy_62e.jpg',\n",
       " '+Ia1w3OBRfG+wt7jSe3LNw_poovy_632.jpg',\n",
       " '.DS_Store',\n",
       " '2FVCA93xQYmDLdep7QmhDg_poovy_629.jpg',\n",
       " '55jx2rgoROSA57h%DCxydQ_divvy_62b.jpg',\n",
       " '5Lhtyh21SDaWmhgzVOsdSQ_divvy_62d.jpg',\n",
       " 'FiRg4dgfTJK5%zyn2bNWAQ_poovy_636.jpg',\n",
       " 'iHPIlCiPQmKu5Pxl6YpgGw_poovy_63b.jpg',\n",
       " 'iSpztfhwQc+TaXsIrhjOsw_divvy_63a.jpg',\n",
       " 'kQeWq2EVRn6EhgHMasQ3Eg_poovy_634.jpg',\n",
       " 'luXr4dzgTAq%Y6p+B6Iafw_vivvy_630.jpg',\n",
       " 'P9Ubq5JZTviNEMvxWyYyUg_villavivvy_62a.jpg',\n",
       " 'PLk59%R0RiOGpB7oLmRdwQ_index_63dvivvy.jpg',\n",
       " 's4kbUx5YQfy9z19qGmPXxg_thumb_631vivvy.jpg',\n",
       " 'sUFwgkMZTumyur0gLg1ySQ_vivvy_635.jpg',\n",
       " 'sujULyDVRxCn20jEWrBfEQ_vivvy_633.jpg',\n",
       " 'UNADJUSTEDNONRAW_divvy_638.jpg',\n",
       " 'UNADJUSTEDNONRAW_divvy_639.jpg',\n",
       " 'WzD8UwhJRDawTTft74AhXA_poovy_62c.jpg',\n",
       " 'xYJXpVP1Q+CLwYRppVx1xQ_poovy_637.jpg',\n",
       " 'zodRD52bSn6VlzxOw+Oh7w_vivvy_62f.jpg']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['zodRD52bSn6VlzxOw+Oh7w_vivvy_62f.jpg',\n",
       " '.DS_Store',\n",
       " 'iHPIlCiPQmKu5Pxl6YpgGw_poovy_63b.jpg',\n",
       " 'kQeWq2EVRn6EhgHMasQ3Eg_poovy_634.jpg',\n",
       " 'xYJXpVP1Q+CLwYRppVx1xQ_poovy_637.jpg',\n",
       " 'P9Ubq5JZTviNEMvxWyYyUg_villavivvy_62a.jpg',\n",
       " 'sujULyDVRxCn20jEWrBfEQ_vivvy_633.jpg',\n",
       " '55jx2rgoROSA57h%DCxydQ_divvy_62b.jpg',\n",
       " '2FVCA93xQYmDLdep7QmhDg_poovy_629.jpg',\n",
       " '5Lhtyh21SDaWmhgzVOsdSQ_divvy_62d.jpg',\n",
       " 'PLk59%R0RiOGpB7oLmRdwQ_index_63dvivvy.jpg',\n",
       " 'sUFwgkMZTumyur0gLg1ySQ_vivvy_635.jpg',\n",
       " 's4kbUx5YQfy9z19qGmPXxg_thumb_631vivvy.jpg',\n",
       " 'iSpztfhwQc+TaXsIrhjOsw_divvy_63a.jpg',\n",
       " 'luXr4dzgTAq%Y6p+B6Iafw_vivvy_630.jpg',\n",
       " 'FiRg4dgfTJK5%zyn2bNWAQ_poovy_636.jpg',\n",
       " '%sGrCwA+SFWLLhXrdXsJnw_divvy_62e.jpg',\n",
       " 'UNADJUSTEDNONRAW_divvy_639.jpg',\n",
       " '+Ia1w3OBRfG+wt7jSe3LNw_poovy_632.jpg',\n",
       " 'WzD8UwhJRDawTTft74AhXA_poovy_62c.jpg',\n",
       " 'UNADJUSTEDNONRAW_divvy_638.jpg']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    shuffle(files)\n",
    "    shuffle(files)\n",
    "    files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "    if int(len(files)*0.01)<100:\n",
    "        split_sample = files[:int(len(files)*0.01)]\n",
    "    else:\n",
    "        split_sample = files[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    # create a sample sub-directory if it doesnt already exist\n",
    "    if not os.path.isdir(os.path.join(sourcepath,'sample')):\n",
    "        os.mkdir(os.path.join(sourcepath,'sample')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    for files_to_copy in split_sample:\n",
    "        shutil.copy2(os.path.join(sourcepath,'train',files_to_copy), os.path.join(sourcepath,'sample'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tying it all into a function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def copy_one_percent_sample(sourcepath):\n",
    "    '''\n",
    "    Image processing is resource intensive. Best practice is to create a sample directory for\n",
    "    training your models with the same directory structure as that of your actual dataset.\n",
    "    \n",
    "    By creating a 1% sample of the entire image dataset (or 100 images which ever is smaller), \n",
    "    yu can build and optimise your models in your laptop without having to run a GPU server.\n",
    "    \n",
    "    Once you're happy with the code, feel free to change the directory your neural net points to.\n",
    "    \n",
    "    That's why its good practice to build a sample dataset. Hope this shit wasnt too patronising! \n",
    "    Its new for me. :)\n",
    "    '''\n",
    "    # the big idea is to copy from the train folder itself. Thats how kaggle is aligning these folders\n",
    "    # Get a list of all the filenames in the directory into a list and randomly shuffle them\n",
    "    # one can see that kaggle puts bulk of the images in the training directory\n",
    "    files = os.listdir(os.path.join(sourcepath,'train'))\n",
    "    \n",
    "    # shuffle them twice so as to get a random split of images in your sample\n",
    "    shuffle(files)\n",
    "    shuffle(files)\n",
    "\n",
    "    # build the list of images that we want to push into our sample dataset\n",
    "    if int(len(files)*0.01)<100:\n",
    "        split_sample = files[:int(len(files)*0.01)]\n",
    "    else:\n",
    "        split_sample = files[:100]\n",
    "    \n",
    "    # create a sample sub-directory if it doesnt already exist. IT SHOULDN'T. CHECK IF IT IS!\n",
    "    if not os.path.isdir(os.path.join(sourcepath,'sample')):\n",
    "        os.mkdir(os.path.join(sourcepath,'sample')) \n",
    "\n",
    "    # use shutil.copy2 method to copy image as well as image metadata into sample directory\n",
    "    for files_to_copy in split_sample:\n",
    "        shutil.copy2(os.path.join(sourcepath,'train',files_to_copy), os.path.join(sourcepath,'sample'))\n",
    "        \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "copy_one_percent_sample(\"data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Tie in all the functions above to split an image classification dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def split_image_dataset_into_train_test_validation_sample(sourcepath,\n",
    "                                                           response_classes,\n",
    "                                                           perc_split = 0.8):\n",
    "    '''\n",
    "    creates a directory structure in the format:\n",
    "                        \t\t\t\t\t    data  \n",
    "                 ____________________________||__________________\n",
    "                 |\t      |\t                |\t                    |\n",
    "               train\ttest  \t          valid              sample\t\t \n",
    "\t          ____|___                   _____|___          ______|_______     \n",
    "              |       |                 |         |        |              |\n",
    "           cats    dogs               cats     dogs    train            valid\n",
    "                                               \t     ___|___          ____|____    \n",
    "                                               \t    |       |        |         |\n",
    "                                             \t    cats   dogs     cats      dogs\n",
    "    \n",
    "    sourcepath is the location that points to the original data download and unzip folder into test and train dataset\n",
    "    '''\n",
    "    \n",
    "    # step1: copy 1% of images or 100 images (whichever is smaller) into a sample folder for initial processing\n",
    "    copy_one_percent_sample(sourcepath)\n",
    "    \n",
    "    # step2: split sample into training and validation datasets (default value 80%:20%)\n",
    "    #        sample step also calls the function to classify into separate folders for each constituent response class\n",
    "    split_into_training_validation_datasets(sourcepath=os.path.join(sourcepath,\"sample\"),\n",
    "                                            response_classes=response_classes, \n",
    "                                            perc_split=perc_split)\n",
    "    \n",
    "    # step3: split train into training and validation datasets (default value 80%:20%)\n",
    "    #        this creates train and valid directories within the train foler itself. need to move it one level up\n",
    "    split_into_training_validation_datasets(sourcepath=os.path.join(sourcepath,\"train\"),\n",
    "                                            response_classes=response_classes, \n",
    "                                            perc_split=perc_split)\n",
    "    \n",
    "    # step4: move train and valid directories within the train folder one level up\n",
    "    os.rename(os.path.join(sourcepath,\"train\",\"valid\"), os.path.join(sourcepath,\"valid\"))\n",
    "    os.rename(os.path.join(sourcepath,\"train\"), os.path.join(sourcepath,\"unprocessed\"))\n",
    "    os.rename(os.path.join(sourcepath,\"unprocessed\",\"train\"), os.path.join(sourcepath,\"train\"))\n",
    "    \n",
    "    return (\"Success\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 files were unable to be classified into response classes in the 'train' sub-directory within the 'data/sample' directory\n",
      "0 files were unable to be classified into response classes in the 'valid' sub-directory within the 'data/sample' directory\n",
      "1 files were unable to be classified into response classes in the 'train' sub-directory within the 'data/train' directory\n",
      "0 files were unable to be classified into response classes in the 'valid' sub-directory within the 'data/train' directory\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Success'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_image_dataset_into_train_test_validation_sample(\"data\", \n",
    "                                                       response_classes=['divvy','poovy','vivvy'],\n",
    "                                                       perc_split = 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python2.7_practicaldl_course",
   "language": "python",
   "name": "practicaldeeplearning_course"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
