{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What I want to do: **First submission gave me a log loss of 0.27459 . This is of course ridiculous, but its ok because the model was trained on the sample data. Can I make it better. Fine tune the model? Also how can I check the validity of my predictions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Admin stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define paths for training, test and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dir = \"data/data_kaggle_dogscats/\"\n",
    "train_path = \"data/data_kaggle_dogscats/train/\"\n",
    "test_path = \"data/data_kaggle_dogscats/test/\"\n",
    "validation_path = \"data/data_kaggle_dogscats/valid/\"\n",
    "sample_train_path = \"data/data_kaggle_dogscats/sample/train/\"\n",
    "sample_validation_path = \"data/data_kaggle_dogscats/sample/valid/\"\n",
    "results_path = \"data/data_kaggle_dogscats/results/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "\n",
    "import os, json\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=4, linewidth=100)\n",
    "from matplotlib import pyplot as plt\n",
    "from datetime import datetime\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jeremy has created a \"utils.py\" file with a whole host of convenience utility functions. I need to familiarise this in greater detail, especially the \"plots\" function. But lets simply use this for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: Tesla K80 (CNMeM is disabled, cuDNN 5103)\n",
      "/home/ubuntu/anaconda2/lib/python2.7/site-packages/theano/sandbox/cuda/__init__.py:600: UserWarning: Your cuDNN version is more recent than the one Theano officially supports. If you see any problems, try updating Theano or downgrading cuDNN to version 5.\n",
      "  warnings.warn(warn)\n",
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import utils; reload(utils)\n",
    "from utils import plots\n",
    "from utils import save_array, load_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 12: Load model weights with full training data 1 epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import\n",
    "import vgg16; reload(vgg16)\n",
    "from vgg16 import Vgg16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda2/lib/python2.7/site-packages/keras/layers/core.py:577: UserWarning: `output_shape` argument not specified for layer lambda_1 and cannot be automatically inferred with the Theano backend. Defaulting to output shape `(None, 3, 224, 224)` (same as input shape). If the expected output shape is different, specify it via the `output_shape` argument.\n",
      "  .format(self.name, input_shape))\n"
     ]
    }
   ],
   "source": [
    "# instantiate\n",
    "vgg = Vgg16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load weights calculated and saved in the previous ipython notebook\n",
    "vgg.model.load_weights(results_path+\"fulltraining.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12500 images belonging to 1 classes.\n",
      "CPU times: user 6min 28s, sys: 1min 11s, total: 7min 40s\n",
      "Wall time: 5min 17s\n"
     ]
    }
   ],
   "source": [
    "# use this to run predictions on the test set\n",
    "%time batches, predict_proba = vgg.test(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filenames = batches.filenames\n",
    "save_array(results_path+\"test_predictions_fulltraining1epoch.dat\", predict_proba)\n",
    "save_array(results_path+\"filenames_fulltraining1epoch.dat\", filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now save the predictions to a new submissions file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# first load the saved test results\n",
    "preds = load_array(results_path+\"test_predictions_fulltraining1epoch.dat\")\n",
    "filenames = load_array(results_path+\"filenames_fulltraining1epoch.dat\")\n",
    "isdog = preds[:,1]\n",
    "isdog_clipped = np.clip(isdog, 0.05,0.95)\n",
    "ids = [int(re.findall(r\"\\d+\", i)[0]) for i in filenames]\n",
    "submission = np.stack([ids,isdog_clipped],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/gitrepo/kaggle_dogs_cats/data/data_kaggle_dogscats/results\n"
     ]
    }
   ],
   "source": [
    "%cd $results_path\n",
    "submission_file_name = 'submission3.csv'\n",
    "np.savetxt(submission_file_name, submission, fmt='%d,%.5f', header='id,label', comments='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This has resulted in a log loss of 0.103, which puts me in the top 30% of the public scores. More tuning is required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 13: Understand the veracity of my predictions using the validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excellent points by Jeremy here. As well as looking at the overall metrics, it's also a good idea to look at examples of each of:\n",
    "1. A few correct labels at random\n",
    "2. A few incorrect labels at random\n",
    "3. The most correct labels of each class (ie those with highest probability that are correct)\n",
    "4. The most incorrect labels of each class (ie those with highest probability that are incorrect)\n",
    "5. The most uncertain labels (ie those with probability closest to 0.5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
